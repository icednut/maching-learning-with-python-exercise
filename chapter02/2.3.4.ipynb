{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.4 나이브 베이즈 분류기\n",
    "- 나이브 베이즈naive bayes 분류기는 선형 모델과 유사 (LogisticRegression, LinearSVC 같은 선형 분류기보다 훈련 속도가 빠르지만 일반화 성능이 조금 뒤짐)\n",
    "- scikit-learn에 구현된 나이브 베이즈 분류기는 3가지: GaussianNB, BernouliNB, MultinomialNB\n",
    "  - Gaussian은 연속적인 데이터에 적용\n",
    "  - BernoulliNB는 이진 데이터에 적용\n",
    "  - MultinomialNB는 카운트 데이터(Feature가 어떤 것을 헤아린 정수 카운트, ex: 문장에 나타난 단어의 횟수)에 적용\n",
    "  - BernoulliNB, MultinomialNB는 대부분 텍스트 데이터를 분류할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB\n",
    "- 각 클래스의 Feature 중 0이 아닌 것이 몇 개인지 셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 1, 0, 1],\n",
    "             [0, 0, 0, 1],\n",
    "             [1, 0, 1, 1],\n",
    "             [1, 0, 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y == [0 1 0 1], label == 0, y == label [ True False  True False], X[y == label] == [[0 1 0 1]\n",
      " [0 0 0 1]]\n",
      "y == [0 1 0 1], label == 1, y == label [False  True False  True], X[y == label] == [[1 0 1 1]\n",
      " [1 0 1 0]]\n",
      "feature count: \n",
      "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # 클래스마다 반복\n",
    "    # 특성마다 1이 나타난 횟수를 센다\n",
    "    counts[label] = X[y == label].sum(axis = 0)\n",
    "    print(f\"y == {y}, label == {label}, y == label {y == label}, X[y == label] == {X[y == label]}\")\n",
    "print(\"feature count: \\n{}\".format(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB, GaussianNB\n",
    "- MultinomialNB: 클래스별로 특성의 평균을 계산\n",
    "- GaussianNB: 클래스별로 각 특성의 표준편차와 평균을 저장\n",
    "- 예측할 땐 데이터 포인트를 클래스의 통계 값과 비교해서 가장 잘 맞는 클래스를 예측 값으로 사용\n",
    "- 나이브 베이즈 모델의 coef_는 기울기 w가 아니라서 선형 모델과는 의미가 다르다?\n",
    "  - MultinomailNB와 BernoulliNB의 coef?는 특성 카운트 수를 로그 변환한 형태\n",
    "  - intercept_는 클래스 카운트를 로그 변환한 것\n",
    "  - 예측을 할 때 선형 함수처럼 데이터 포인트에 coef_를 곲하고 intercept_를 더하여 클래스에 속할 확률을 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 나이브 베이즈의 장단점과 매개변수\n",
    "- MultinomialNB, BernoulliNB 매개변수: alpha (복잡도 조절 역할)\n",
    "    - alpha가 주어지면 알고리즘이 모든 특성에 양의 값을 가진 가상의 데이터 포인트를 alpha 개수만큼 추가 -> 통계 데이터를 완만하게 만듬\n",
    "    - alpha가 크면 더 완만해지고 모델의 복잡도는 낮아짐\n",
    "- GaussianNB: 고차원 데이터 셋에서 사용\n",
    "- Multinomial, BernoulliNB: 텍스트 같은 희소한 데이터를 카운트하는데 사용\n",
    "    - MultinomailNB는 0이 아닌 특성이 비교적 많은 대용량 데이터 셋에서 성능이 좋음\n",
    "    \n",
    "### 나이브 베이즈 장단점\n",
    "- 장점\n",
    "    - 훈련과 예측 속도가 빠르며, 훈련 과정을 이해하기 쉽다.\n",
    "    - 희소한 고차원 데이터에서 잘 작동\n",
    "    - 비교적 매개변수에 민감하지 않음\n",
    "    - 선형 모델로\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
